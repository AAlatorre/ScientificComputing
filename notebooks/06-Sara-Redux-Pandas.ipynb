{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sara the Turtle: `Pandas` Redux\n",
    "Recalling the script we wrote earlier to process the ARGOS tracking data for Sara the Turtle. Here, we'll demonstrate how `pandas`, can greatly simplify that task. More specifically, we look at the ability of Pandas to extract and summarize data stored in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the user date\n",
    "userDate = '7/3/2003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the location classes\n",
    "locClasses = '1','2','3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the modules\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set a variable to the path where the tracking data lives\n",
    "dataFilename = '..\\Data\\Sara.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the csv file into a dataFrame\n",
    "Here we pull the data into the data frame. We need to set a few extra parameters in the `read_csv` statement because of the format of our \"csv\" file. \n",
    "* First, we need to skip the comment lines, done with the `comment=` argument. \n",
    "* Second we need to specify the delimiter since it isn't the default of a comma. \n",
    "* Third, these data have a 'uid' column with values that uniquely identify each column; we'll use this as the index of our dataFrame rather than the default of sequentially increasing integers. This is done with the `index_col` argument. \n",
    "* And lastly, we have a few numeric columns that should be imported as strings since they represent nominal values. We do this by setting up a data type dictionary which specifies columns who's data types we want to override from default types when importing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the data as a pandas dataframe. \n",
    "df = pd.read_csv(dataFilename,\n",
    "                 comment='#',   #Skip lines that start with '#'\n",
    "                 delimiter='\\t', #Set the delimiter as <tab>\n",
    "                 index_col='uid',\n",
    "                 dtype={'uid':'str','tag_id':'str'}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the data types in what we just imported..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting a subset of the data\n",
    "Subsetting data in Pandas is a two step process. The first step is to create a **row mask**. This is a single column of boolean values with rows corresponding to whether a criteria we specify is True or False. Below, we'll create a row mask of values that match the user provided date. There are more sophisitcated ways of dealing with dates, but for our purposes selecting rows that start with the date string the user provides works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mask of user dates\n",
    "dateMask = df['utc'].str.startswith(userDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create another row mask, this time selecting rows with `lc` values within our list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a mask of location classe\n",
    "lcMask = df['lc'].isin(locClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we *apply* these masks, using the bit-wise `&` to select and returnrows where *both* masks are true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter the records that match the above masks\n",
    "dfOutput = df[dateMask & lcMask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many rows meet this criteria\n",
    "print len(dfOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"On {}, Sara the turtle was found at:\".format(userDate)\n",
    "for i,row in dfOutput.iterrows():\n",
    "    print('  {0} deg Lat; {1} deg lon'.format(row['lat1'],row['lon1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
